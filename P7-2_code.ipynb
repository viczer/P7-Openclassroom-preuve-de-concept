{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parcours Ingénieur Machine Learning**<br>\n",
    "**Plus d'informations** : https://openclassrooms.com/fr/paths/148-ingenieur-machine-learning <br>\n",
    "\n",
    "**Auteur** : Viktoriya Zeruk<br>\n",
    "**Date dernière version** : 08/08/2022<br>\n",
    "**Accès projet git** : https://github.com/viczer/P7-Openclassroom-preuve-de-concept\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"display: flex; background: rgb(75,0,130);\n",
    "background: linear-gradient(90deg, rgba(75,0,130,1) 47%, rgba(216,191,216,1) 89%, rgba(230,230,250,1) 100%);\">\n",
    "<h2 style=\"margin: auto; font-weight: bold; padding: 30px 30px 0px 30px;\" align=\"center\">| Project 7 : Développez une preuve de concept | <br></h2> </div>    \n",
    "\n",
    "<div style=\"display: flex; background: rgb(75,0,130);\n",
    "background: linear-gradient(90deg, rgba(75,0,130,1) 47%, rgba(216,191,216,1) 89%, rgba(230,230,250,1) 100%);\">\n",
    "<h4 style=\"margin: auto; font-weight: bold; padding: 30px 30px 0px 30px;\" align=\"center\"> </h4> \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Contexte\n",
    "\n",
    "Lors d'un précédent projet (Project 6 : Classez des images à l'aide d'algorithmes de Deep Learning), nous avions utilisé le réseau de neurones convolutif.\n",
    "Le meilleur modèle du projet précédent : *Xception*\n",
    "\n",
    "## Les données\n",
    " Stanford Dogs Dataset\n",
    "\n",
    "## Mission\n",
    "Tester une nouvelle méthode ViT (Vision Transformer) concurrente au  méthode CNN. \n",
    "L'idée est donc de comparer, en termes de précision et temps de calcul, un CNN et un ViT. Pour ce faire, nous réutiliserons le Stanford Dogs Dataset et le modèle Xception précédemment entraîné que l'on comparera au modèle ViT-B/16 de Google.\n",
    "\n",
    "## Ressources de calcul\n",
    "L'entraînement (même partiel) d'un réseau de neurones convolutionnels est très gourmand en ressources.\n",
    " Solutions :\n",
    "\n",
    "* Limiter le jeu de données, en ne sélectionnant que quelques classes (races de chiens), ce qui permettra déjà de tester la démarche et la conception des modèles, avant une éventuelle généralisation.\n",
    "* Utiliser la carte graphique de l’ordinateur en tant que GPU (l'installation est un peu fastidieuse, et l'ordinateur est inutilisable le temps du calcul)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "* <a href=\"#C1\">I - Importation des données</a>\n",
    "* <a href=\"#C2\">II - Méthodes de classification d'images</a>\n",
    "    * <a href=\"#C3\">Xception</a>\n",
    "    * <a href=\"#C4\">Vision Transformer</a>\n",
    "* <a href=\"#C5\">III - Conclusion</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"C1\">I - Importation des données</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T19:02:35.036359Z",
     "iopub.status.busy": "2022-07-13T19:02:35.035747Z",
     "iopub.status.idle": "2022-07-13T19:02:44.973361Z",
     "shell.execute_reply": "2022-07-13T19:02:44.972245Z",
     "shell.execute_reply.started": "2022-07-13T19:02:35.036323Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import timeit\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imread\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset, load_dataset, load_metric\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.xception import Xception\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GlobalAveragePooling2D, Dense\n",
    "\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:06:57.771662Z",
     "iopub.status.busy": "2022-07-13T12:06:57.771297Z",
     "iopub.status.idle": "2022-07-13T12:09:26.779204Z",
     "shell.execute_reply": "2022-07-13T12:09:26.778255Z",
     "shell.execute_reply.started": "2022-07-13T12:06:57.771630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 2.34 s, total: 1min 8s\n",
      "Wall time: 2min 28s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uri</th>\n",
       "      <th>breed</th>\n",
       "      <th>nb_color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/stanford-dogs-dataset/images/Images/n...</td>\n",
       "      <td>otterhound</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/stanford-dogs-dataset/images/Images/n...</td>\n",
       "      <td>otterhound</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/stanford-dogs-dataset/images/Images/n...</td>\n",
       "      <td>otterhound</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/stanford-dogs-dataset/images/Images/n...</td>\n",
       "      <td>otterhound</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/stanford-dogs-dataset/images/Images/n...</td>\n",
       "      <td>otterhound</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 uri       breed  nb_color\n",
       "0  ../input/stanford-dogs-dataset/images/Images/n...  otterhound         3\n",
       "1  ../input/stanford-dogs-dataset/images/Images/n...  otterhound         3\n",
       "2  ../input/stanford-dogs-dataset/images/Images/n...  otterhound         3\n",
       "3  ../input/stanford-dogs-dataset/images/Images/n...  otterhound         3\n",
       "4  ../input/stanford-dogs-dataset/images/Images/n...  otterhound         3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#On récupère les uri de chaque image\n",
    "dogs_lst = []\n",
    "for file in glob.glob('../input/images/images/*/*.jpg'):\n",
    "    dogs_lst.append(file)\n",
    "\n",
    "#On les stocke dans un dataframe\n",
    "data = pd.DataFrame(dogs_lst, columns=['uri'])\n",
    "\n",
    "#On récupère la race du chien\n",
    "data['breed'] = data['uri'].apply(lambda x: x.split('/')[-2].split('-')[-1])\n",
    "\n",
    "#On récupère le nombre de canaux utilisés\n",
    "data['nb_color'] = data['uri'].apply(lambda x: imread(x).shape[2])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:10:50.424272Z",
     "iopub.status.busy": "2022-07-13T12:10:50.423846Z",
     "iopub.status.idle": "2022-07-13T12:10:50.534124Z",
     "shell.execute_reply": "2022-07-13T12:10:50.533011Z",
     "shell.execute_reply.started": "2022-07-13T12:10:50.424238Z"
    }
   },
   "outputs": [],
   "source": [
    "#Exportation du jeu de donées\n",
    "data.to_csv('data.csv', sep = ',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T19:02:44.976019Z",
     "iopub.status.busy": "2022-07-13T19:02:44.975320Z",
     "iopub.status.idle": "2022-07-13T19:02:45.054427Z",
     "shell.execute_reply": "2022-07-13T19:02:45.053566Z",
     "shell.execute_reply.started": "2022-07-13T19:02:44.975981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uri</th>\n",
       "      <th>breed</th>\n",
       "      <th>nb_color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/stanford-dogs-dataset/images/Images/n...</td>\n",
       "      <td>otterhound</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/stanford-dogs-dataset/images/Images/n...</td>\n",
       "      <td>otterhound</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/stanford-dogs-dataset/images/Images/n...</td>\n",
       "      <td>otterhound</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/stanford-dogs-dataset/images/Images/n...</td>\n",
       "      <td>otterhound</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/stanford-dogs-dataset/images/Images/n...</td>\n",
       "      <td>otterhound</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 uri       breed  nb_color\n",
       "0  ../input/stanford-dogs-dataset/images/Images/n...  otterhound         3\n",
       "1  ../input/stanford-dogs-dataset/images/Images/n...  otterhound         3\n",
       "2  ../input/stanford-dogs-dataset/images/Images/n...  otterhound         3\n",
       "3  ../input/stanford-dogs-dataset/images/Images/n...  otterhound         3\n",
       "4  ../input/stanford-dogs-dataset/images/Images/n...  otterhound         3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chargement du jeu de données\n",
    "data = pd.read_csv('../input/data/data.csv', sep=',',encoding='utf-8')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une première chose à vérifier est la présence éventuelle de valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T19:02:57.096796Z",
     "iopub.status.busy": "2022-07-13T19:02:57.096449Z",
     "iopub.status.idle": "2022-07-13T19:02:57.117824Z",
     "shell.execute_reply": "2022-07-13T19:02:57.116892Z",
     "shell.execute_reply.started": "2022-07-13T19:02:57.096767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uri         0\n",
       "breed       0\n",
       "nb_color    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:10:06.341089Z",
     "iopub.status.busy": "2022-07-13T12:10:06.340383Z",
     "iopub.status.idle": "2022-07-13T12:10:06.354227Z",
     "shell.execute_reply": "2022-07-13T12:10:06.353141Z",
     "shell.execute_reply.started": "2022-07-13T12:10:06.341049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    20579\n",
       "4        1\n",
       "Name: nb_color, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['nb_color'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une des photos dans le jeu de données comporte, en plus des trois couches RGB, une couche Alpha, qui permet de coder la transparence de l'image. Toutefois, cette couche n'est souvent pas compatibles avec les modèles de traitement d'image. On va donc la retirer pour éviter de potentiels soucis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:13:11.284277Z",
     "iopub.status.busy": "2022-07-13T12:13:11.283333Z",
     "iopub.status.idle": "2022-07-13T12:13:11.299445Z",
     "shell.execute_reply": "2022-07-13T12:13:11.298340Z",
     "shell.execute_reply.started": "2022-07-13T12:13:11.284240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    20579\n",
       "Name: nb_color, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = data[data['nb_color']==4].index[0]\n",
    "data.drop(data.index[idx], axis=0, inplace=True)\n",
    "data['nb_color'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:13:23.815024Z",
     "iopub.status.busy": "2022-07-13T12:13:23.813891Z",
     "iopub.status.idle": "2022-07-13T12:13:23.824698Z",
     "shell.execute_reply": "2022-07-13T12:13:23.823360Z",
     "shell.execute_reply.started": "2022-07-13T12:13:23.814981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le jeu de données comporte 20579 photos de chiens, représentant 119 races différentes\n"
     ]
    }
   ],
   "source": [
    "nb_pictures = len(data)\n",
    "nb_breeds = len(data['breed'].unique())\n",
    "print(f\"Le jeu de données comporte {nb_pictures} photos de chiens, représentant {nb_breeds} races différentes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"C2\">II -  Méthodes de classification d'images</a>\n",
    "\n",
    "Notre problématique est de trouver une méthode de classification d'images de chiens en fonction de la race. Nous comparerons pour ce faire une méthode CNN et une méthode Vision Transformer. La comparaison se fera grâce au score Accuracy et au temps de calcul.</br>\n",
    "Commençons par séparer notre jeu de données en jeu d'entraînement et jeu de test de sorte à entraîner les modèles à partir des mêmes données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:15:07.833211Z",
     "iopub.status.busy": "2022-07-13T12:15:07.832245Z",
     "iopub.status.idle": "2022-07-13T12:15:07.949187Z",
     "shell.execute_reply": "2022-07-13T12:15:07.948215Z",
     "shell.execute_reply.started": "2022-07-13T12:15:07.833170Z"
    }
   },
   "outputs": [],
   "source": [
    "#Séparation jeux entraînement/test\n",
    "train, test = train_test_split(data, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "#Exportation des jeux d'entraînement et de test\n",
    "train.to_csv('train.csv', sep = ',', encoding='utf-8', index=False)\n",
    "test.to_csv('test.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T14:18:00.193123Z",
     "iopub.status.busy": "2022-07-13T14:18:00.192124Z",
     "iopub.status.idle": "2022-07-13T14:18:00.303454Z",
     "shell.execute_reply": "2022-07-13T14:18:00.302454Z",
     "shell.execute_reply.started": "2022-07-13T14:18:00.193084Z"
    }
   },
   "outputs": [],
   "source": [
    "#Chargement du jeu d'entraînement\n",
    "train = pd.read_csv('../input/data/train.csv', sep=',',encoding='utf-8')\n",
    "\n",
    "#Chargement du jeu de test\n",
    "test = pd.read_csv('../input/data/test.csv', sep=',',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T16:05:01.257376Z",
     "iopub.status.busy": "2022-07-13T16:05:01.256766Z",
     "iopub.status.idle": "2022-07-13T16:05:01.269310Z",
     "shell.execute_reply": "2022-07-13T16:05:01.268400Z",
     "shell.execute_reply.started": "2022-07-13T16:05:01.257339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temps entraînement</th>\n",
       "      <th>accuracy train</th>\n",
       "      <th>accuracy test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Xception</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vit</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Temps entraînement accuracy train accuracy test\n",
       "Xception                NaN            NaN           NaN\n",
       "Vit                     NaN            NaN           NaN"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tableau de comparaison des modèles\n",
    "df_res = pd.DataFrame([], index=['Xception', 'Vit'], columns=['Temps entraînement', 'accuracy train', 'accuracy test'])\n",
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"C3\">Xception</a>\n",
    "\n",
    "Le modèle Xception est un réseau de neurones convolutifs profond développé en 2017 par Google. Sa principale caractéristique est qu'il dispose de couches de convolution profondes, alternatives aux couches de convolution classiques qui ont pour but de réduire les temps de calcul. Il est également constitué de connexions récurrentes, c'est-à-dire des boucles de rétropropagation qui permettent de conserver en mémoire des informations obtenues lors d'étapes précédentes et de les utiliser au moment de prendre des décisions dans les étapes suivantes.</br>\n",
    "<img src=\"https://www.researchgate.net/publication/343535666/figure/fig2/AS:930842713014272@1598941605167/Architecture-of-Xception-model-obtained-from-Chollet-2017.png\"> <br>\n",
    "Nous l'entraînerons en utilisant un fine-tuning partiel, en ré-entraînant 10% des couches convolutionnelles hautes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T14:18:09.063834Z",
     "iopub.status.busy": "2022-07-13T14:18:09.063467Z",
     "iopub.status.idle": "2022-07-13T14:18:09.071466Z",
     "shell.execute_reply": "2022-07-13T14:18:09.070237Z",
     "shell.execute_reply.started": "2022-07-13T14:18:09.063802Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data_cnn(train, test):\n",
    "    \"\"\"Fonction qui prend en entrée le train et le test et renvoie les données préparées pour le modèle Xception\"\"\"\n",
    "    \n",
    "    #On redimensionne les images\n",
    "    datagen = image.ImageDataGenerator(rescale=1./255)\n",
    "    train_gen = datagen.flow_from_dataframe(train, \n",
    "                                            x_col='uri', \n",
    "                                            y_col='breed', \n",
    "                                            target_size=(224, 224),\n",
    "                                            seed=42)\n",
    "    test_gen = datagen.flow_from_dataframe(test, \n",
    "                                            x_col='uri', \n",
    "                                            y_col='breed', \n",
    "                                            target_size=(224, 224),\n",
    "                                            seed=42)\n",
    "\n",
    "    return (train_gen, test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:17:06.306890Z",
     "iopub.status.busy": "2022-07-13T12:17:06.306150Z",
     "iopub.status.idle": "2022-07-13T12:17:06.316770Z",
     "shell.execute_reply": "2022-07-13T12:17:06.315738Z",
     "shell.execute_reply.started": "2022-07-13T12:17:06.306849Z"
    }
   },
   "outputs": [],
   "source": [
    "def cnn_fine_tuning(nb_breeds):\n",
    "    \"\"\"Fonction qui prend en entrée le nombre de races utilisées\n",
    "    et renvoie le modèle Xception prêt pour le transfer learning avec fine-tuning partiel\"\"\"\n",
    "\n",
    "    #Charger le modèle pré-entraîné sans les couches fully-connected\n",
    "    model = Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    #Entraîner 10% des couches hautes\n",
    "    nb_10 = int(len(model.layers)*0.9)\n",
    "    for layer in model.layers[:nb_10]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[nb_10:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    #Définir le nouveau modèle\n",
    "    new_model = Sequential()\n",
    "    new_model.add(model)\n",
    "    new_model.add(GlobalAveragePooling2D())\n",
    "    new_model.add(Dense(256, activation='relu'))\n",
    "    new_model.add(Dense(nb_breeds, activation='softmax'))\n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T14:18:15.966247Z",
     "iopub.status.busy": "2022-07-13T14:18:15.965491Z",
     "iopub.status.idle": "2022-07-13T14:18:52.005558Z",
     "shell.execute_reply": "2022-07-13T14:18:52.004639Z",
     "shell.execute_reply.started": "2022-07-13T14:18:15.966206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16463 validated image filenames belonging to 119 classes.\n",
      "Found 4116 validated image filenames belonging to 119 classes.\n",
      "CPU times: user 639 ms, sys: 738 ms, total: 1.38 s\n",
      "Wall time: 36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_cnn, test_cnn = prepare_data_cnn(train, test)\n",
    "breeds_dict = (train_cnn.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T12:20:08.978142Z",
     "iopub.status.busy": "2022-07-13T12:20:08.977785Z",
     "iopub.status.idle": "2022-07-13T13:34:31.838270Z",
     "shell.execute_reply": "2022-07-13T13:34:31.837082Z",
     "shell.execute_reply.started": "2022-07-13T12:20:08.978113Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 12:20:10.759083: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-07-13 12:20:14.843585: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    }
   ],
   "source": [
    "#Création et entrainement du modèle\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "cnn = cnn_fine_tuning(nb_breeds)\n",
    "cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "cnn_res = cnn.fit(train_cnn, epochs=50, verbose=0)\n",
    "\n",
    "time = timeit.default_timer() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T13:34:31.840777Z",
     "iopub.status.busy": "2022-07-13T13:34:31.840377Z",
     "iopub.status.idle": "2022-07-13T13:34:58.794385Z",
     "shell.execute_reply": "2022-07-13T13:34:58.793392Z",
     "shell.execute_reply.started": "2022-07-13T13:34:31.840736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/129 [==============================] - 27s 200ms/step - loss: 2.0960 - accuracy: 0.7272\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temps entraînement</th>\n",
       "      <th>accuracy train</th>\n",
       "      <th>accuracy test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Xception</th>\n",
       "      <td>4462.854993</td>\n",
       "      <td>0.987791</td>\n",
       "      <td>0.727162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vit</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Temps entraînement accuracy train accuracy test\n",
       "Xception        4462.854993       0.987791      0.727162\n",
       "Vit                     NaN            NaN           NaN"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On récupère les scores\n",
    "df_res.loc['Xception', 'Temps entraînement'] = time\n",
    "df_res.loc['Xception', 'accuracy train'] = cnn_res.history['accuracy'][-1]\n",
    "df_res.loc['Xception', 'accuracy test'] = cnn.evaluate(test_cnn)[1]\n",
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"C4\">Vision Transformer</a>\n",
    "\n",
    "Un Vision Transformer n'est autre que l'application d'un Transformer, couramment utilisé en traitement du langage, pour le traitement d'images. Les Transformers mesurent les relations entre les paires de tokens d'entrée (mots dans le cas de chaînes de texte), appelées attention. Le coût est quadratique par rapport au nombre de tokens. Pour les images, l'unité de base de l'analyse est le pixel. Cependant, le calcul des relations pour chaque paire de pixels dans une image typique est prohibitif en termes de mémoire et de calcul. Au lieu de cela, ViT calcule les relations entre les pixels dans diverses petites sections de l'image (par exemple, 16x16 pixels pour le modèle ViT-B/16 que l'on va utiliser), à un coût considérablement réduit. Les sections (avec les indices de position) sont placées dans une séquence, puis réduites linéairement afin de réduire la taille. Les séquences/tokens ainsi obtenues peuvent alors être passées en entrée d'un Transformer pour la classification.<br>\n",
    "<img src=\"https://viso.ai/wp-content/uploads/2021/09/vision-transformer-vit.png\"> <br>\n",
    "\n",
    "Les ViT peuvent donner de meilleurs résultats que les CNN à condition d'avoir été entraînés sur des jeux de données très conséquent. Le modèle ViT-B/16 utilisé à été pré-entraîné sur le jeu de données ImageNet-21k contenant plus de 14 millions d'images. Notre jeu de données étant petit, on va utiliser une méthode de fine-tuning sur le modèle pré-entraîné disponible sur la plateforme HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T14:11:50.410053Z",
     "iopub.status.busy": "2022-07-13T14:11:50.409304Z",
     "iopub.status.idle": "2022-07-13T14:11:50.415789Z",
     "shell.execute_reply": "2022-07-13T14:11:50.414622Z",
     "shell.execute_reply.started": "2022-07-13T14:11:50.410014Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_breed_num(breed):\n",
    "    \"\"\"Fonction qui renvoie le numéro correspondant à la race passée en argument\"\"\"\n",
    "    return breeds_dict[breed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T14:13:06.095275Z",
     "iopub.status.busy": "2022-07-13T14:13:06.094938Z",
     "iopub.status.idle": "2022-07-13T14:13:06.119223Z",
     "shell.execute_reply": "2022-07-13T14:13:06.118139Z",
     "shell.execute_reply.started": "2022-07-13T14:13:06.095246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.1 ms, sys: 2.05 ms, total: 19.2 ms\n",
      "Wall time: 18.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_vit = train.copy()\n",
    "test_vit = test.copy()\n",
    "\n",
    "train_vit['breed_num'] = train_vit['breed'].apply(lambda x: get_breed_num(x))\n",
    "\n",
    "test_vit['breed_num'] = test_vit['breed'].apply(lambda x: get_breed_num(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T14:13:41.057005Z",
     "iopub.status.busy": "2022-07-13T14:13:41.056324Z",
     "iopub.status.idle": "2022-07-13T14:13:41.170624Z",
     "shell.execute_reply": "2022-07-13T14:13:41.169631Z",
     "shell.execute_reply.started": "2022-07-13T14:13:41.056965Z"
    }
   },
   "outputs": [],
   "source": [
    "#Exportation des jeux d'entraînement et de test\n",
    "train_vit.to_csv('train_vit.csv', sep = ',', encoding='utf-8', index=False)\n",
    "test_vit.to_csv('test_vit.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T14:19:10.942555Z",
     "iopub.status.busy": "2022-07-13T14:19:10.942175Z",
     "iopub.status.idle": "2022-07-13T14:19:11.539105Z",
     "shell.execute_reply": "2022-07-13T14:19:11.538111Z",
     "shell.execute_reply.started": "2022-07-13T14:19:10.942521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-cf13d0b5eea460fb/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e38ce50413a444889966c9fe37398e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48edd2874f3a45ef9fed6c1ae36959e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-cf13d0b5eea460fb/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57dea76d588642ccaa1c8adc9578df3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Chargement des jeux d'entraînement et de test dans un dataset Hugging Face\n",
    "dataset = load_dataset('csv', data_files={'train': '../input/data/train_vit.csv', 'test': '../input/data/test_vit.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T14:15:18.056376Z",
     "iopub.status.busy": "2022-07-13T14:15:18.056020Z",
     "iopub.status.idle": "2022-07-13T14:15:18.062982Z",
     "shell.execute_reply": "2022-07-13T14:15:18.062055Z",
     "shell.execute_reply.started": "2022-07-13T14:15:18.056344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['uri', 'breed', 'nb_color', 'breed_num'],\n",
       "        num_rows: 16463\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['uri', 'breed', 'nb_color', 'breed_num'],\n",
       "        num_rows: 4116\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque les modèles ViT sont entraînés, des transformations spécifiques sont appliquées aux images qui leur sont fournies. Pour s'assurer que l'on applique les bonnes transformations, nous utiliserons un ViTFeatureExtractor initialisé avec une configuration qui a été sauvegardée avec le modèle pré-entraîné que nous prévoyons d'utiliser. Dans notre cas, nous utiliserons le modèle google/vit-base-patch16-224-in21k et chargerons son extracteur de caractéristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T14:19:18.116370Z",
     "iopub.status.busy": "2022-07-13T14:19:18.116019Z",
     "iopub.status.idle": "2022-07-13T14:19:18.880515Z",
     "shell.execute_reply": "2022-07-13T14:19:18.878341Z",
     "shell.execute_reply.started": "2022-07-13T14:19:18.116339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7204340ba4044a04ba3101188c590394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T14:19:20.538711Z",
     "iopub.status.busy": "2022-07-13T14:19:20.538338Z",
     "iopub.status.idle": "2022-07-13T14:19:20.548271Z",
     "shell.execute_reply": "2022-07-13T14:19:20.547094Z",
     "shell.execute_reply.started": "2022-07-13T14:19:20.538678Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform(example_batch):\n",
    "    \"\"\"Fonction en entrée un DictDataset des images et les prépare pour le modèle ViT\"\"\"\n",
    "    \n",
    "    inputs = feature_extractor([Image.open(x) for x in example_batch['uri']], return_tensors='pt')\n",
    "    inputs['labels'] = example_batch['breed_num']\n",
    "    return inputs\n",
    "\n",
    "prepared_ds = dataset.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T14:19:22.730364Z",
     "iopub.status.busy": "2022-07-13T14:19:22.729652Z",
     "iopub.status.idle": "2022-07-13T14:19:22.735356Z",
     "shell.execute_reply": "2022-07-13T14:19:22.734449Z",
     "shell.execute_reply.started": "2022-07-13T14:19:22.730326Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T14:19:25.353437Z",
     "iopub.status.busy": "2022-07-13T14:19:25.353075Z",
     "iopub.status.idle": "2022-07-13T14:19:26.054659Z",
     "shell.execute_reply": "2022-07-13T14:19:26.053725Z",
     "shell.execute_reply.started": "2022-07-13T14:19:25.353403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a20b132f63441f94a485e3b826598d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut à présent télécharger le modèle en veillant à bien préciser le nombre de classes utilisées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T14:19:29.423502Z",
     "iopub.status.busy": "2022-07-13T14:19:29.423165Z",
     "iopub.status.idle": "2022-07-13T14:19:39.847602Z",
     "shell.execute_reply": "2022-07-13T14:19:39.846566Z",
     "shell.execute_reply.started": "2022-07-13T14:19:29.423471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee46ec6809146d8aafd8eeffeacdde0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423e7fa3fc1f4e0da533de12a11058c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/330M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(breeds_dict),\n",
    "    id2label={v: k for v, k in enumerate(breeds_dict)},\n",
    "    label2id=breeds_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T14:19:43.159393Z",
     "iopub.status.busy": "2022-07-13T14:19:43.159018Z",
     "iopub.status.idle": "2022-07-13T14:19:43.250153Z",
     "shell.execute_reply": "2022-07-13T14:19:43.249167Z",
     "shell.execute_reply.started": "2022-07-13T14:19:43.159360Z"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./vit-base-beans-demo-v5\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=4,\n",
    "  fp16=True,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T14:20:23.875139Z",
     "iopub.status.busy": "2022-07-13T14:20:23.874788Z",
     "iopub.status.idle": "2022-07-13T14:20:29.164936Z",
     "shell.execute_reply": "2022-07-13T14:20:29.163857Z",
     "shell.execute_reply.started": "2022-07-13T14:20:23.875110Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"test\"],\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T14:21:48.960316Z",
     "iopub.status.busy": "2022-07-13T14:21:48.959584Z",
     "iopub.status.idle": "2022-07-13T15:37:55.337800Z",
     "shell.execute_reply": "2022-07-13T15:37:55.336685Z",
     "shell.execute_reply.started": "2022-07-13T14:21:48.960274Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 16463\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4116\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4116' max='4116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4116/4116 1:15:57, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.785700</td>\n",
       "      <td>3.747199</td>\n",
       "      <td>0.539845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.990100</td>\n",
       "      <td>2.946486</td>\n",
       "      <td>0.611273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.261000</td>\n",
       "      <td>2.304302</td>\n",
       "      <td>0.631195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.900800</td>\n",
       "      <td>1.876223</td>\n",
       "      <td>0.668367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.661000</td>\n",
       "      <td>1.615720</td>\n",
       "      <td>0.683431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.477500</td>\n",
       "      <td>1.435463</td>\n",
       "      <td>0.682945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.309900</td>\n",
       "      <td>1.243238</td>\n",
       "      <td>0.713557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.139600</td>\n",
       "      <td>1.205417</td>\n",
       "      <td>0.714529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.984500</td>\n",
       "      <td>1.066272</td>\n",
       "      <td>0.735180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.098400</td>\n",
       "      <td>1.032939</td>\n",
       "      <td>0.732507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.559300</td>\n",
       "      <td>0.975173</td>\n",
       "      <td>0.740039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.944874</td>\n",
       "      <td>0.748542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.617800</td>\n",
       "      <td>0.942117</td>\n",
       "      <td>0.743440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.532300</td>\n",
       "      <td>0.955489</td>\n",
       "      <td>0.740282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.525800</td>\n",
       "      <td>0.882871</td>\n",
       "      <td>0.749757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.366600</td>\n",
       "      <td>0.902644</td>\n",
       "      <td>0.746356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.494700</td>\n",
       "      <td>0.917441</td>\n",
       "      <td>0.748056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.748100</td>\n",
       "      <td>0.829363</td>\n",
       "      <td>0.759475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.439800</td>\n",
       "      <td>0.840138</td>\n",
       "      <td>0.762634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.502900</td>\n",
       "      <td>0.810191</td>\n",
       "      <td>0.774052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.342700</td>\n",
       "      <td>0.854047</td>\n",
       "      <td>0.766764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.190200</td>\n",
       "      <td>0.867847</td>\n",
       "      <td>0.764577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.165700</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>0.780855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.287300</td>\n",
       "      <td>0.801646</td>\n",
       "      <td>0.783042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.291100</td>\n",
       "      <td>0.794641</td>\n",
       "      <td>0.784014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.251000</td>\n",
       "      <td>0.830435</td>\n",
       "      <td>0.773081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.100200</td>\n",
       "      <td>0.768621</td>\n",
       "      <td>0.790816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.191800</td>\n",
       "      <td>0.748714</td>\n",
       "      <td>0.794704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.283900</td>\n",
       "      <td>0.752832</td>\n",
       "      <td>0.802235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>0.769914</td>\n",
       "      <td>0.796161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.719291</td>\n",
       "      <td>0.810253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.725533</td>\n",
       "      <td>0.804179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>0.741367</td>\n",
       "      <td>0.808552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.739847</td>\n",
       "      <td>0.805394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.175500</td>\n",
       "      <td>0.732117</td>\n",
       "      <td>0.811710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.717329</td>\n",
       "      <td>0.814383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>0.715526</td>\n",
       "      <td>0.818513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>0.712284</td>\n",
       "      <td>0.820457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.705357</td>\n",
       "      <td>0.822643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.696757</td>\n",
       "      <td>0.824344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.110200</td>\n",
       "      <td>0.696847</td>\n",
       "      <td>0.826045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-100\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-100/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-100/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-100/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-200\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-200/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-200/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-200/preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-300\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-300/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-300/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-300/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-400\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-400/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-400/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-400/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-500\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-500/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-500/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-500/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-600\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-600/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-600/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-600/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-700\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-700/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-700/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-700/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-800\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-800/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-800/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-800/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-900\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-900/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-900/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-900/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-1000\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-1000/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-1000/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-1000/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-1100\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-1100/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-1100/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-1100/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-1200\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-1200/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-1200/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-1200/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-1300\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-1300/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-1300/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-1300/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-1100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-1400\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-1400/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-1400/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-1400/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-1500\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-1500/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-1500/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-1500/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-1300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-1600\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-1600/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-1600/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-1600/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-1700\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-1700/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-1700/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-1700/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-1800\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-1800/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-1800/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-1800/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-1900\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-1900/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-1900/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-1900/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-1700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-2000\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-2000/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-2000/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-2000/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-1800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-2100\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-2100/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-2100/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-2100/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-1900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-2200\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-2200/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-2200/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-2200/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-2100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-2300\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-2300/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-2300/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-2300/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-2200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-2400\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-2400/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-2400/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-2400/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-2500\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-2500/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-2500/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-2500/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-2300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-2600\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-2600/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-2600/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-2600/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-2400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-2700\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-2700/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-2700/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-2700/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-2500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-2800\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-2800/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-2800/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-2800/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-2600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-2900\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-2900/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-2900/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-2900/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-2700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-3000\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-3000/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-3000/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-3000/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-2900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-3100\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-3100/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-3100/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-3100/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-2800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-3200\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-3200/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-3200/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-3200/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-3300\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-3300/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-3300/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-3300/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-3200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-3400\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-3400/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-3400/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-3400/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-3300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-3800\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-3800/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-3800/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-3800/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-3600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-3900\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-3900/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-3900/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-3900/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-3700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-4000\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-4000/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-4000/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-4000/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-3800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5/checkpoint-4100\n",
      "Configuration saved in ./vit-base-beans-demo-v5/checkpoint-4100/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/checkpoint-4100/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/checkpoint-4100/preprocessor_config.json\n",
      "Deleting older checkpoint [vit-base-beans-demo-v5/checkpoint-3900] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./vit-base-beans-demo-v5/checkpoint-4000 (score: 0.6967569589614868).\n",
      "Saving model checkpoint to ./vit-base-beans-demo-v5\n",
      "Configuration saved in ./vit-base-beans-demo-v5/config.json\n",
      "Model weights saved in ./vit-base-beans-demo-v5/pytorch_model.bin\n",
      "Feature extractor saved in ./vit-base-beans-demo-v5/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =          4.0\n",
      "  total_flos               = 4757525103GF\n",
      "  train_loss               =       0.7147\n",
      "  train_runtime            =   1:16:05.06\n",
      "  train_samples_per_second =       14.425\n",
      "  train_steps_per_second   =        0.902\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()\n",
    "\n",
    "time = timeit.default_timer() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T15:37:55.349186Z",
     "iopub.status.busy": "2022-07-13T15:37:55.347119Z",
     "iopub.status.idle": "2022-07-13T15:37:55.357043Z",
     "shell.execute_reply": "2022-07-13T15:37:55.355863Z",
     "shell.execute_reply.started": "2022-07-13T15:37:55.349137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4566.369979034001\n"
     ]
    }
   ],
   "source": [
    "print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T15:45:50.705551Z",
     "iopub.status.busy": "2022-07-13T15:45:50.704955Z",
     "iopub.status.idle": "2022-07-13T15:46:50.135602Z",
     "shell.execute_reply": "2022-07-13T15:46:50.134682Z",
     "shell.execute_reply.started": "2022-07-13T15:45:50.705514Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4116\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2573' max='515' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [515/515 07:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        4.0\n",
      "  eval_accuracy           =     0.8243\n",
      "  eval_loss               =     0.6968\n",
      "  eval_runtime            = 0:00:59.40\n",
      "  eval_samples_per_second =     69.288\n",
      "  eval_steps_per_second   =      8.669\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(prepared_ds['test'])\n",
    "trainer.log_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T15:48:32.468447Z",
     "iopub.status.busy": "2022-07-13T15:48:32.467503Z",
     "iopub.status.idle": "2022-07-13T15:53:35.061321Z",
     "shell.execute_reply": "2022-07-13T15:53:35.060353Z",
     "shell.execute_reply.started": "2022-07-13T15:48:32.468410Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16463\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        4.0\n",
      "  eval_accuracy           =     0.9922\n",
      "  eval_loss               =     0.0388\n",
      "  eval_runtime            = 0:05:02.58\n",
      "  eval_samples_per_second =     54.408\n",
      "  eval_steps_per_second   =      6.801\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(prepared_ds['train'])\n",
    "trainer.log_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T16:06:03.774602Z",
     "iopub.status.busy": "2022-07-13T16:06:03.774191Z",
     "iopub.status.idle": "2022-07-13T16:06:03.786090Z",
     "shell.execute_reply": "2022-07-13T16:06:03.785031Z",
     "shell.execute_reply.started": "2022-07-13T16:06:03.774564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temps entraînement</th>\n",
       "      <th>accuracy train</th>\n",
       "      <th>accuracy test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Xception</th>\n",
       "      <td>4462.854993</td>\n",
       "      <td>0.987791</td>\n",
       "      <td>0.727162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vit</th>\n",
       "      <td>4566.369979</td>\n",
       "      <td>0.9922</td>\n",
       "      <td>0.8243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Temps entraînement accuracy train accuracy test\n",
       "Xception        4462.854993       0.987791      0.727162\n",
       "Vit             4566.369979         0.9922        0.8243"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On récupère les scores\n",
    "df_res.loc['Vit', 'Temps entraînement'] = time\n",
    "df_res.loc['Vit', 'accuracy train'] = 0.9922 \n",
    "df_res.loc['Vit', 'accuracy test'] = 0.8243\n",
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"C5\">III - Conclusion</a>\n",
    "\n",
    "Le modèle ViT utilise l'auto-attention multi-têtes en vision par ordinateur sans nécessiter les biais spécifiques à l'image. Le modèle divise les images en une série de patchs d'intégration positionnels, qui sont traités par l'encodeur du transformateur. Il le fait pour comprendre les caractéristiques locales et globales que possède l'image. Enfin, le ViT a un taux de précision plus élevé sur un grand ensemble de données avec un temps de formation réduit.\n",
    " \n",
    "L’inconvénient d’un ViT est qu’il nécessite plus de données d’entraînement qu’un CNN.\n",
    "\n",
    "Les deux modèles ont des temps de calcul similaires mais un score de précision significativement meilleur pour le ViT. Le modèle ViT est toutefois plus compliqué à mettre en oeuvre, notamment si l'on souhaite optimiser les hyperparamètres ou ajouter de la Data Augmentation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
